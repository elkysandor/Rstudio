---

title: '52414: Home Exam by 207047259'

output:

  html_document: default

  pdf_document: default

date: "July 124th, 2021"

---







```{r, echo = FALSE, results = 'hide', warning=FALSE, message=FALSE}

library(ggplot2)

library(tidyverse)

library(rvest)

library(dplyr)

library(reshape)

library(data.table)

library(caTools)

library(plotly)

data(stop_words)



options(scipen=999)

```






**Solutions:**



```{r, echo = FALSE, results = 'hide', warning=FALSE, message=FALSE}

library(stringr)

library(tidyr)

library(tidyverse)

library(tidytext) 

library(dplyr)

library(reshape2)

library(chron) # for dealing with times 

library(wordcloud2) # package for drawing word-cloud

library(usmap) # Show USA map

library(stringi)
#set.seed(12345)
```
## Q1

# a
With two strong properties of the *exp* distribution first *memoryless* and that the minimum of independent  exp random variable is *exp* with parameter equal to sum of the parameters of exp random variables we took minimum on.I thought on the simulation that every time randomized a shot with minimum distribution of *exp* that will represents a shot that was taken and kill someone on the rival team. And by that keep randomized first shot's(with change parameter because the number of exp distribution is decreasing when someone is dead) until one team has zero mans alive and then ill count the number of alive persons in the **winning team**.


# b

here i simulate a shoot out with n statistican
```{r}
shootuot<-function(n) {
  team1<-n
  team2<-n
  while (team1>0&team2>0) {
  shooter1<-rexp(1,team1)
  shooter2<-rexp(1,team2)
  if(shooter1<shooter2){
    team2<-team2-1
  }
  else {
    team1=team1-1
  }
  }
  ifelse(team1==0,team2,team1)
}
```

Here i iterate the shoot out
```{r}

iteration<-function(iters,num_of_static) {
remain_life<-rep(0,iters)
for(i in c(1:iters)){
  remain_life[i] <- shootuot(num_of_static)
}
return(remain_life)
}
mean_test<-mean(iteration(1000,10))
var_test<-var(iteration(1000,10))
```

By the simulation we got that E[X]=`r mean_test` and V[X]=`r var_test`

# c


```{r}

start<-10
vec_of_means<-rep(0,log(10240,2)-log(10,2)+1)
for(i in c(1:length(vec_of_means))){
  vec_of_means[i]<-mean(iteration(100,start))
  start<-start*2
}

increasePower  <- function(v) { v ^ (0:(length(v)-1)) }
n_vec<-10*increasePower(rep(2,length(vec_of_means)))
par(mfcol=c(1,2))
plot(x=n_vec,y=vec_of_means,xlab="number of shooters",
     ylab="estimate mean of Xn")
plot(x=n_vec,y=vec_of_means,log="xy",xlab="log scaled number of shooters",
     ylab="log scaled estimate mean of Xn",pch=8)
mtext("killing other team", side = 3, line = -1, outer = TRUE,cex = 1.25,font = 2)
abline(a = 0,b = 0.75,col="red")

```


Above we can see two plot *left* plot is the estimate vs number of shooters in each team and the 
*right* plot is a scatter plot of estimate vs number of shooters with log scale on both axis(x and y)
and the red line i added on that plot is the line $Y=0.75 x$ which we can see is very similar to the dot's
so we get $log(E[x])=0.75log(n)$ which from properties of log function we get the function
$f(n)=n^{\frac{3}{4}}$ holds that $\mu_n \approx f(n)$

# d


shooting you own team simulation
```{r}

modi_shootuot<-function(n) {
  team1<-n
  team2<-n
  while (team1>0&team2>0) {
  shooter1<-rexp(1,team1)
  shooter2<-rexp(1,team2)
  if(shooter1<shooter2){team1<-team1-1}
  else {team2<-team2-1}
  }
  ifelse(team1==0,team2,team1)
}

mody_iteration<-function(iters,num_of_static) {
remain_life<-rep(0,iters)
for(i in c(1:iters)){remain_life[i] <- modi_shootuot(num_of_static)}
return(remain_life)
}



start_2<-10
vec_of_means_2<-rep(0,log(10240,2)-log(10,2)+1)
for(i in c(1:length(vec_of_means_2))){
  vec_of_means_2[i]<-mean(mody_iteration(100,start_2))
  start_2<-start_2*2
}

mean_2<-round(mean(vec_of_means_2),3)
var_2<-round(var(vec_of_means_2),3)
plot(x=n_vec,y=vec_of_means_2,xlab="number of shooters",
     ylab="estimate mean of Xn",main="killing my team")
legend("bottomright",
       legend = c(paste("mean",as.character(mean(mean_2))), paste("var",as.character(var_2))))

```


Here we can see that the estimator is not dependent on the number of shooters which is big difference from the previous sun question and this is because if we will look at the denstyu graph of exp distribution we will notice that more the parameter is higher the density close to zero is higher and the slope is steep which mean that the probability to sample close to zero is higher and for that in C we have dependent on the number of shooters because the team that kills first have better chances to kill first again and so on. and here we have the opposite because every team every team reduce the chance to shot first and that why we see those result
and we can see that we have scatter around 2 so $f(n)=2$ holds that $\mu_n \approx f(n)$ in this case


# e


```{r}

drunk_shootuot<-function(n) {
  team1<-n
  team2<-n
  while (team1>0&team2>0) {
  shooter1<-rexp(1,team1)
  shooter2<-rexp(1,team2)
  if(shooter1<shooter2){
    drunk_shot<-rbernoulli(1,team2/(team1+team2))
    if (drunk_shot) {team2<-team2-1}
    else {team1<-team1-1}
  }else {
    drunk_shot<-rbernoulli(1,team1/(team1+team2))
    if (drunk_shot) {team1<-team1-1}
    else {team2<-team2-1}
  }
  }
  ifelse(team1==0,team2,team1)
}

drunk_iteration<-function(iters,num_of_static) {
remain_life_3<-rep(0,iters)
for(i in c(1:iters)){remain_life_3[i] <- drunk_shootuot(num_of_static)}
return(remain_life_3)
}


start_3<-10
vec_of_means_3<-rep(0,log(10240,2)-log(10,2)+1)
for(i in c(1:length(vec_of_means_3))){
  vec_of_means_3[i]<-mean(drunk_iteration(100,start_3))
  start_3<-start_3*2
}
mean_3<-round(mean(vec_of_means_3),3)
var_3<-round(var(vec_of_means_3),3)
plot(x=n_vec,y=vec_of_means_3,xlab="number of shooters",
     ylab="estimate mean of Xn",main="drunk teams")
legend("bottomright",
       legend = c(paste("mean",as.character(mean(mean_3))), paste("var",as.character(var_3))))
```


We have here similar result and this is because the chance to kill some one from the team with more alive people is higher because of the distribution of the bernoulli so that as in the previous case number of alive shooters is decreasing in joint way in each team and there for we get $f(n)=2$ holds that $\mu_n \approx f(n)$ in this case too

# f


```{r}
zombi_shootuot<-function(n) {
  team1<-n
  team2<-n
  while (team1>0&team2>0) {
  shooter1<-rexp(1,n)
  shooter2<-rexp(1,n)
  if(shooter1<shooter2){team1<-team1-1}
  else { team2<-team2-1}
  }
  ifelse(team1==0,team2,team1)
}

zombi_iteration<-function(iters,num_of_static) {
remain_life_4<-rep(0,iters)
for(i in c(1:iters)){remain_life_4[i] <- zombi_shootuot(num_of_static)}
return(remain_life_4)
}



start_4<-10
vec_of_means_4<-rep(0,log(10240,2)-log(10,2)+1)
for(i in c(1:length(vec_of_means_4))){
  vec_of_means_4[i]<-mean(zombi_iteration(100,start_4))
  start_4<-start_4*2
}
mean_4<-round(mean(vec_of_means_4),3)
var_4<-round(var(vec_of_means_4),3)
par(mfcol=c(1,2))
plot(x=n_vec,y=vec_of_means_4,xlab="number of shooters",
     ylab="estimate mean of Xn")
plot(x=n_vec,y=vec_of_means_4,log="xy",xlab="log scaled number of shooters",
     ylab="log scaled estimate mean of Xn",pch=8)
mtext("zombie fight", side = 3, line = -1, outer = TRUE,cex = 1.25,font = 2)
abline(a=0,b=0.5,col="red")

```


So here i presented two graphs with in the same way as i did in *c* with little difference that this time
the red line is $Y=0.5 x$ and therefor we got that $f(n)=n^{\frac{1}{2}}$ holds that $\mu_n \approx f(n)$
so the difference from the last two sub question is that now we have dependent on the number of shooters 
and the difference from *c* is that here our dependent in the number of shooter is weaker which we can see from the lower slope of our red line here(compared to c).

## Q2

# a


Read the data
```{r}

tweets<-read_csv("/Users/elkysandor/Downloads/crowdflower-2015-new-years-resolutions/data/new_years_resolutions_dfe.csv")

```
As we can see every column is in the right class

```{r}
nice_print<-within(tweets[1:2,], {text = paste(substr(text, 1, 15),
                                               "...", sep = "")})
knitr::kable(nice_print) 
```


```{r}
size<-dim(tweets)
nice_print_2<-within(tweets[(size[1]-1):size[1],], {text = paste(substr(text, 1, 15),
                                               "...", sep = "")})
knitr::kable(nice_print_2) 
```

# b


```{r}
only_time<- sub(".* ", "", tweets$tweet_created)
time_date<-as.POSIXct(only_time,format="%H:%M")
tms<-times(format(time_date, "%H:%M:%S"))
time<-mutate(tweets,"times"=tms)
ggplot(time,aes(x=times))+geom_histogram(binwidth = 1/24,boundary=0,color = "black",fill="skyblue",)+
  scale_x_continuous(breaks = round(seq(0,1,1/24),4))+
  theme(axis.text.x = element_text(face = "bold", size = 10, angle = 60, hjust = 1))

```


Here every bin is exactly one hour in the numeric representation so we can see that the hours with most tweets are 9-11 AM and 12-14 and the hours with the fewest hours are 2-4 AM 

# c


```{r,warning=FALSE}
to_plot_len<-dplyr::select(tweets,c(gender,tweet_region,resolution_category,text))
to_plot_len<-mutate(to_plot_len,length = nchar(tweets$text))
ggplot(to_plot_len,aes(x=length,y=gender,color=gender))+geom_boxplot()+stat_summary(fun="mean")
```


We can see from the graph that the difference between male and female is little with female have a little bit longest tweets on average.

```{r,warning=FALSE}
ggplot(to_plot_len,aes(x=length,y=tweet_region,color=tweet_region))+geom_boxplot()+stat_summary(fun="mean")
```


There is not major difference in the length of the tweets per region
```{r,warning=FALSE}
ggplot(to_plot_len,aes(x=length,y=resolution_category,color=resolution_category))+geom_boxplot()+stat_summary(fun="mean")
```


The longest tweets are from philanthropic category(by average). and the smallest are from Humor((by average))


# d


Used regex to remove what was asked to remove
```{r}
long_regex<-c("[^ ]*[#][^ ]*","[^ ]*@[^ ]*","[^ ]*&[^ ]*","[^ ]*-[^ ]*",
"[^ ]*[.][^ ]*","[^ ]*:[^ ]*","[^ ]*[?][^ ]*","[^ ]*_[^ ]*")
only_lower<-str_to_lower(tweets$text)
long_df<-data.frame(x=unlist(str_split(only_lower,"\\s+")))
long_df$x<-as.character(long_df$x)
no_stop<-anti_join(long_df,stop_words,by=c("x"="word"))
for (i in long_regex) {
  no_stop$x<-str_remove_all(no_stop$x,i)
}
no_bad_word<-gsub("\\bresolution\\b|\\brt\\b|\\b2015\\b","",no_stop$x)
only_words<-str_remove_all(no_bad_word,"[^ ]*[,!][^ ]*|[^ ]*[\\d][^ ]*|[^ ]*=[^ ]*")
ss <- data.frame(x=only_words)
sss <- setDT(ss)[, .(freq = .N), x]
sss<-arrange(sss,desc(freq))
sss<-sss[-1,]
top_100_words<-head(sss,100)
wordcloud2(data = top_100_words,shape = "diamond")
```

# e


Here my frequency table is *big mat* and *delta_fric* is the table of the measure D 
```{r}
words_only<-as.character(top_100_words$x)
for_num_per<-dplyr::select(tweets,c(tweet_region,resolution_category,retweet_count,))
freq_cat<-as.data.frame(table(tweets$resolution_category))
colnames(freq_cat)[1]<-"category"
freq_cat$category<-as.character(freq_cat$category)
big_mat<-matrix(nrow = 100,ncol = 10)
for (i in 1:length(words_only)) {
  for (j in 1:length(freq_cat$category)) {
    per_cat<-filter(tweets,resolution_category==freq_cat$category[j])
    text_only<-per_cat[["text"]]
    num_of_apper<-length(grep(words_only[i],text_only))
    big_mat[i,j]<-num_of_apper
  }
}
for (i in 1:dim(freq_cat)[1]) {big_mat[,i]<-big_mat[,i]/freq_cat$Freq[i]}
delta_fric<-matrix(nrow = 100,ncol = 10)
for (i in 1:100) {
  for (j in 1:10) {delta_fric[i,j]<-big_mat[i,j]-max(big_mat[i,][-j])}
}
most_charc<-df <- matrix(nrow = 10,ncol = 4)
for (j in 1:10) {
  chosen<-words_only[which(delta_fric[,j]%in%sort(delta_fric[,j],decreasing = TRUE)[1:3])]
  most_charc[j,]<-c(freq_cat$category[j],chosen)
}
most_charc<-as.data.frame(most_charc)
colnames(most_charc)<-c("category","word1","word2","word3")
knitr::kable(most_charc)
```
As we can see the words make sense with the categories

# f

```{r,warning=FALSE}
for_num_per<-dplyr::select(tweets,c(tweet_region,resolution_category))
per_cat<-group_by(for_num_per,resolution_category)%>%count()
per_cat_reg<-group_by(for_num_per,tweet_region,resolution_category)%>%count()
ggplot(per_cat,aes(x=resolution_category,y=n,color=resolution_category))+
  geom_point()+ylab("number of tweet's")+
  geom_text(label=as.character(per_cat$n), 
    nudge_x = 0.3, nudge_y = 0.5, 
    check_overlap = T)+coord_flip()+ylim(c(0,2000))

```


```{r,warning=FALSE}
table_by_rg<-dcast(per_cat_reg, resolution_category~tweet_region, value.var="n")
knitr::kable(table_by_rg)
```


# g 


```{r}
e_dot_j<-as.numeric(apply(table_by_rg[,2:5], 2, sum))
e_i_dot<-as.numeric(apply(table_by_rg[,2:5], 1, sum))
e_dot_dot<-sum(e_dot_j)
statis_S<-c()
for (i in 1:10) {
  for (j in 1:4) {
    eij<-(e_dot_j[j]*e_i_dot[i])/e_dot_dot
    statis_S<-c(statis_S,((table_by_rg[i,j+1]-eij)^2)/eij)
  }
}
statis_S<-sum(statis_S)
p_val_1<-1-pchisq(statis_S,27)
```

We got p_val equal to `r round(p_val_1,3)` which is high p_val which mean we wont reject the null hypothesis

```{r}
for_num_per_2<-dplyr::select(tweets,c(gender,resolution_category))
per_gen<-group_by(for_num_per_2,gender,resolution_category)%>%count()
table_by_gn<-dcast(per_gen, resolution_category~gender, value.var="n")
e_dot_j_2<-as.numeric(apply(table_by_gn[,2:3], 2, sum))
e_i_dot_2<-as.numeric(apply(table_by_gn[,2:3], 1, sum))
e_dot_dot_2<-sum(e_dot_j_2)
statis_S_2<-c()
for (i in 1:10) {
  for (j in 1:2) {
    eij_2<-(e_dot_j_2[j]*e_i_dot_2[i])/e_dot_dot_2
    statis_S_2<-c(statis_S_2,((table_by_gn[i,j+1]-eij_2)^2)/eij_2)
  }
}
statis_S_2<-sum(statis_S_2)
p_val_2<-1-pchisq(statis_S_2,9)


```

We got p_val so small that R return p_val equals to `r p_val_2` which mean that from the chi squared test 
we got significant difference in the distributions of categories between males and females.

# h

```{r}
for_chi<-dplyr::select(tweets,resolution_category,tweet_region)
all_S<-c()
for (i in 1:1000) {
  for_chi$tweet_region<-for_chi$tweet_region[sample(1:dim(for_chi)[1])]
  per_cat_reg_2<-group_by(for_chi,tweet_region,resolution_category)%>%count()
  table_by_rg_2<-dcast(as.data.table(per_cat_reg_2), resolution_category~tweet_region, value.var="n")
  table_by_rg_2<-as.data.frame(table_by_rg_2)
  e_dot_j<-as.numeric(apply(table_by_rg_2[,2:5], 2, sum))
  e_i_dot<-as.numeric(apply(table_by_rg_2[,2:5], 1, sum))
  e_dot_dot<-sum(e_dot_j)
  statis_S_3<-c()
  for (i in 1:10) {
    for (j in 1:4) {
      eij<-(e_dot_j[j]*e_i_dot[i])/e_dot_dot
      statis_S_3<-c(statis_S_3,((table_by_rg_2[i,j+1]-eij)^2)/eij)
    }
  }
  statis_S_3<-sum(statis_S_3)
  all_S<-c(all_S,statis_S_3)
}
all_S<-as.data.frame(all_S)


ggplot(all_S,aes(all_S))+geom_density(color="blue",fill="skyblue")+stat_function(fun = dchisq, args = list(df = 27),color="red")
numeric_pval<-length(all_S$all_S[which(all_S$all_S>statis_S)])/1000
```
 As we can see the distributions are similar
 and we got that the empirical p_val is `r numeric_pval` and the difference is `r round(abs(numeric_pval-p_val_1),4)`
 which pretty low

```{r}
per_stat<-tweets%>% dplyr::select(tweet_state)%>%group_by(tweet_state)%>%count()
full_pic<-full_join(statepop,per_stat,by = c("abbr"="tweet_state") )
full_pic$pop_2015<-full_pic$pop_2015/1000000
colnames(full_pic)[4]<-"pop_2015_per_M"
full_pic<-mutate(full_pic,tweet_per_M=n/pop_2015_per_M)
full_pic<-full_pic[-which(full_pic$abbr=="DC"),]
pic<-plot_usmap(data = full_pic,exclude ="DC" ,values = "tweet_per_M", color = "red",labels = TRUE) + 
  scale_fill_continuous(
    low = "white", high = "red", name = "tweet per million", label = scales::comma
  )+theme(legend.position = "right") 
pic$layers[[2]]$aes_params$size <- 1.5
print(pic)

```

As we can see the three Countries with maximal number of tweets per million are
*New York*, *Alaska* and *Massachusetts*
and the three Countries with minimal number of tweets per million are
*North Dakota*, *Montana* and *Delaware*





